# global parameters
model_name: textcnn  # 'han', 'fasttext', 'attbilstm', 'textcnn', 'transformer'
                     # refer to README.md for more info about each model

# dataset parameters
dataset: yahoo_answers  # 'ag_news', 'dbpedia', 'yelp_review_polarity', 'yelp_review_full', 'yahoo_answers', 'amazon_review_polarity', 'amazon_review_full'
                        # refer to README.md for more info about each dataset
dataset_path: /data/torch/datasets/text/yahoo_answers_csv  # folder with dataset
output_path: /data/torch/datasets/text/preprocess/yahoo_answers/sents  # folder with data files saved by preprocess.py

# preprocess parameters
word_limit: 200
min_word_count: 5

# word embeddings parameters
emb_pretrain: True  # false: initialize embedding weights randomly
                    # true: load pre-trained word embeddings
emb_folder: /data/embeddings  # only makes sense when `emb_pretrain: True`
emb_filename: glove.6B.300d.txt  # only makes sense when `emb_pretrain: True`
emb_size: 256  # word embedding size
               # only makes sense when `emb_pretrain: False`
fine_tune_word_embeddings: True  # fine-tune word embeddings?

# model parameters
conv_layer: '2D' # '1D', '2D', use 1D or 2D convolution layer
n_kernels: 100
kernel_sizes: [3, 4, 5]
n_channels: 2
dropout: 0.3  # dropout

# checkpoint saving parameters
checkpoint_path: null  # path to save checkpoints, null if never save checkpoints
checkpoint_basename: checkpoint_textcnn_yahoo  # basename of the checkpoint

# training parameters
start_epoch: 0  # start at this epoch
batch_size: 512  # batch size
lr: 0.001  # learning rate
lr_decay: 0.7  # a factor to multiply learning rate with (0, 1)
workers: 64  # number of workers for loading data in the DataLoader
num_epochs: 10  # number of epochs to run
grad_clip: null  # clip gradients at this value, null if never clip gradients
print_freq: 200  # print training status every __ batches
checkpoint: null  # path to model checkpoint, null if none
# tensorboard
tensorboard: False  # enable tensorboard or not?
log_dir: logs/textcnn  # folder for saving logs for tensorboard
                                                              # only makes sense when `tensorboard: True`
reduced_train_size: 1
aug: agnews_sges
exp:
  dir: experiments/tc_ya